{"ast":null,"code":"/**\n * @typedef {import('micromark-util-types').Token} Token\n * @typedef {import('micromark-util-types').Chunk} Chunk\n * @typedef {import('micromark-util-types').Event} Event\n */\nimport { splice } from 'micromark-util-chunked';\n/**\n * Tokenize subcontent.\n *\n * @param {Event[]} events\n * @returns {boolean}\n */\n\nexport function subtokenize(events) {\n  /** @type {Record<string, number>} */\n  const jumps = {};\n  let index = -1;\n  /** @type {Event} */\n\n  let event;\n  /** @type {number|undefined} */\n\n  let lineIndex;\n  /** @type {number} */\n\n  let otherIndex;\n  /** @type {Event} */\n\n  let otherEvent;\n  /** @type {Event[]} */\n\n  let parameters;\n  /** @type {Event[]} */\n\n  let subevents;\n  /** @type {boolean|undefined} */\n\n  let more;\n\n  while (++index < events.length) {\n    while (index in jumps) {\n      index = jumps[index];\n    }\n\n    event = events[index]; // Add a hook for the GFM tasklist extension, which needs to know if text\n    // is in the first content of a list item.\n\n    if (index && event[1].type === 'chunkFlow' && events[index - 1][1].type === 'listItemPrefix') {\n      subevents = event[1]._tokenizer.events;\n      otherIndex = 0;\n\n      if (otherIndex < subevents.length && subevents[otherIndex][1].type === 'lineEndingBlank') {\n        otherIndex += 2;\n      }\n\n      if (otherIndex < subevents.length && subevents[otherIndex][1].type === 'content') {\n        while (++otherIndex < subevents.length) {\n          if (subevents[otherIndex][1].type === 'content') {\n            break;\n          }\n\n          if (subevents[otherIndex][1].type === 'chunkText') {\n            subevents[otherIndex][1]._isInFirstContentOfListItem = true;\n            otherIndex++;\n          }\n        }\n      }\n    } // Enter.\n\n\n    if (event[0] === 'enter') {\n      if (event[1].contentType) {\n        Object.assign(jumps, subcontent(events, index));\n        index = jumps[index];\n        more = true;\n      }\n    } // Exit.\n    else if (event[1]._container) {\n      otherIndex = index;\n      lineIndex = undefined;\n\n      while (otherIndex--) {\n        otherEvent = events[otherIndex];\n\n        if (otherEvent[1].type === 'lineEnding' || otherEvent[1].type === 'lineEndingBlank') {\n          if (otherEvent[0] === 'enter') {\n            if (lineIndex) {\n              events[lineIndex][1].type = 'lineEndingBlank';\n            }\n\n            otherEvent[1].type = 'lineEnding';\n            lineIndex = otherIndex;\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (lineIndex) {\n        // Fix position.\n        event[1].end = Object.assign({}, events[lineIndex][1].start); // Switch container exit w/ line endings.\n\n        parameters = events.slice(lineIndex, index);\n        parameters.unshift(event);\n        splice(events, lineIndex, index - lineIndex + 1, parameters);\n      }\n    }\n  }\n\n  return !more;\n}\n/**\n * Tokenize embedded tokens.\n *\n * @param {Event[]} events\n * @param {number} eventIndex\n * @returns {Record<string, number>}\n */\n\nfunction subcontent(events, eventIndex) {\n  const token = events[eventIndex][1];\n  const context = events[eventIndex][2];\n  let startPosition = eventIndex - 1;\n  /** @type {number[]} */\n\n  const startPositions = [];\n  const tokenizer = token._tokenizer || context.parser[token.contentType](token.start);\n  const childEvents = tokenizer.events;\n  /** @type {[number, number][]} */\n\n  const jumps = [];\n  /** @type {Record<string, number>} */\n\n  const gaps = {};\n  /** @type {Chunk[]} */\n\n  let stream;\n  /** @type {Token|undefined} */\n\n  let previous;\n  let index = -1;\n  /** @type {Token|undefined} */\n\n  let current = token;\n  let adjust = 0;\n  let start = 0;\n  const breaks = [start]; // Loop forward through the linked tokens to pass them in order to the\n  // subtokenizer.\n\n  while (current) {\n    // Find the position of the event for this token.\n    while (events[++startPosition][1] !== current) {// Empty.\n    }\n\n    startPositions.push(startPosition);\n\n    if (!current._tokenizer) {\n      stream = context.sliceStream(current);\n\n      if (!current.next) {\n        stream.push(null);\n      }\n\n      if (previous) {\n        tokenizer.defineSkip(current.start);\n      }\n\n      if (current._isInFirstContentOfListItem) {\n        tokenizer._gfmTasklistFirstContentOfListItem = true;\n      }\n\n      tokenizer.write(stream);\n\n      if (current._isInFirstContentOfListItem) {\n        tokenizer._gfmTasklistFirstContentOfListItem = undefined;\n      }\n    } // Unravel the next token.\n\n\n    previous = current;\n    current = current.next;\n  } // Now, loop back through all events (and linked tokens), to figure out which\n  // parts belong where.\n\n\n  current = token;\n\n  while (++index < childEvents.length) {\n    if ( // Find a void token that includes a break.\n    childEvents[index][0] === 'exit' && childEvents[index - 1][0] === 'enter' && childEvents[index][1].type === childEvents[index - 1][1].type && childEvents[index][1].start.line !== childEvents[index][1].end.line) {\n      start = index + 1;\n      breaks.push(start); // Help GC.\n\n      current._tokenizer = undefined;\n      current.previous = undefined;\n      current = current.next;\n    }\n  } // Help GC.\n\n\n  tokenizer.events = []; // If there’s one more token (which is the cases for lines that end in an\n  // EOF), that’s perfect: the last point we found starts it.\n  // If there isn’t then make sure any remaining content is added to it.\n\n  if (current) {\n    // Help GC.\n    current._tokenizer = undefined;\n    current.previous = undefined;\n  } else {\n    breaks.pop();\n  } // Now splice the events from the subtokenizer into the current events,\n  // moving back to front so that splice indices aren’t affected.\n\n\n  index = breaks.length;\n\n  while (index--) {\n    const slice = childEvents.slice(breaks[index], breaks[index + 1]);\n    const start = startPositions.pop();\n    jumps.unshift([start, start + slice.length - 1]);\n    splice(events, start, 2, slice);\n  }\n\n  index = -1;\n\n  while (++index < jumps.length) {\n    gaps[adjust + jumps[index][0]] = adjust + jumps[index][1];\n    adjust += jumps[index][1] - jumps[index][0] - 1;\n  }\n\n  return gaps;\n}","map":{"version":3,"sources":["/Users/dragos/Ruby-developer/GitHubDev/muse-ant-design-dashboard/node_modules/micromark-util-subtokenize/index.js"],"names":["splice","subtokenize","events","jumps","index","event","lineIndex","otherIndex","otherEvent","parameters","subevents","more","length","type","_tokenizer","_isInFirstContentOfListItem","contentType","Object","assign","subcontent","_container","undefined","end","start","slice","unshift","eventIndex","token","context","startPosition","startPositions","tokenizer","parser","childEvents","gaps","stream","previous","current","adjust","breaks","push","sliceStream","next","defineSkip","_gfmTasklistFirstContentOfListItem","write","line","pop"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA,SAAQA,MAAR,QAAqB,wBAArB;AAEA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,SAASC,WAAT,CAAqBC,MAArB,EAA6B;AAClC;AACA,QAAMC,KAAK,GAAG,EAAd;AACA,MAAIC,KAAK,GAAG,CAAC,CAAb;AACA;;AAEA,MAAIC,KAAJ;AACA;;AAEA,MAAIC,SAAJ;AACA;;AAEA,MAAIC,UAAJ;AACA;;AAEA,MAAIC,UAAJ;AACA;;AAEA,MAAIC,UAAJ;AACA;;AAEA,MAAIC,SAAJ;AACA;;AAEA,MAAIC,IAAJ;;AAEA,SAAO,EAAEP,KAAF,GAAUF,MAAM,CAACU,MAAxB,EAAgC;AAC9B,WAAOR,KAAK,IAAID,KAAhB,EAAuB;AACrBC,MAAAA,KAAK,GAAGD,KAAK,CAACC,KAAD,CAAb;AACD;;AAEDC,IAAAA,KAAK,GAAGH,MAAM,CAACE,KAAD,CAAd,CAL8B,CAKR;AACtB;;AAEA,QACEA,KAAK,IACLC,KAAK,CAAC,CAAD,CAAL,CAASQ,IAAT,KAAkB,WADlB,IAEAX,MAAM,CAACE,KAAK,GAAG,CAAT,CAAN,CAAkB,CAAlB,EAAqBS,IAArB,KAA8B,gBAHhC,EAIE;AACAH,MAAAA,SAAS,GAAGL,KAAK,CAAC,CAAD,CAAL,CAASS,UAAT,CAAoBZ,MAAhC;AACAK,MAAAA,UAAU,GAAG,CAAb;;AAEA,UACEA,UAAU,GAAGG,SAAS,CAACE,MAAvB,IACAF,SAAS,CAACH,UAAD,CAAT,CAAsB,CAAtB,EAAyBM,IAAzB,KAAkC,iBAFpC,EAGE;AACAN,QAAAA,UAAU,IAAI,CAAd;AACD;;AAED,UACEA,UAAU,GAAGG,SAAS,CAACE,MAAvB,IACAF,SAAS,CAACH,UAAD,CAAT,CAAsB,CAAtB,EAAyBM,IAAzB,KAAkC,SAFpC,EAGE;AACA,eAAO,EAAEN,UAAF,GAAeG,SAAS,CAACE,MAAhC,EAAwC;AACtC,cAAIF,SAAS,CAACH,UAAD,CAAT,CAAsB,CAAtB,EAAyBM,IAAzB,KAAkC,SAAtC,EAAiD;AAC/C;AACD;;AAED,cAAIH,SAAS,CAACH,UAAD,CAAT,CAAsB,CAAtB,EAAyBM,IAAzB,KAAkC,WAAtC,EAAmD;AACjDH,YAAAA,SAAS,CAACH,UAAD,CAAT,CAAsB,CAAtB,EAAyBQ,2BAAzB,GAAuD,IAAvD;AACAR,YAAAA,UAAU;AACX;AACF;AACF;AACF,KAtC6B,CAsC5B;;;AAEF,QAAIF,KAAK,CAAC,CAAD,CAAL,KAAa,OAAjB,EAA0B;AACxB,UAAIA,KAAK,CAAC,CAAD,CAAL,CAASW,WAAb,EAA0B;AACxBC,QAAAA,MAAM,CAACC,MAAP,CAAcf,KAAd,EAAqBgB,UAAU,CAACjB,MAAD,EAASE,KAAT,CAA/B;AACAA,QAAAA,KAAK,GAAGD,KAAK,CAACC,KAAD,CAAb;AACAO,QAAAA,IAAI,GAAG,IAAP;AACD;AACF,KAND,CAME;AANF,SAOK,IAAIN,KAAK,CAAC,CAAD,CAAL,CAASe,UAAb,EAAyB;AAC5Bb,MAAAA,UAAU,GAAGH,KAAb;AACAE,MAAAA,SAAS,GAAGe,SAAZ;;AAEA,aAAOd,UAAU,EAAjB,EAAqB;AACnBC,QAAAA,UAAU,GAAGN,MAAM,CAACK,UAAD,CAAnB;;AAEA,YACEC,UAAU,CAAC,CAAD,CAAV,CAAcK,IAAd,KAAuB,YAAvB,IACAL,UAAU,CAAC,CAAD,CAAV,CAAcK,IAAd,KAAuB,iBAFzB,EAGE;AACA,cAAIL,UAAU,CAAC,CAAD,CAAV,KAAkB,OAAtB,EAA+B;AAC7B,gBAAIF,SAAJ,EAAe;AACbJ,cAAAA,MAAM,CAACI,SAAD,CAAN,CAAkB,CAAlB,EAAqBO,IAArB,GAA4B,iBAA5B;AACD;;AAEDL,YAAAA,UAAU,CAAC,CAAD,CAAV,CAAcK,IAAd,GAAqB,YAArB;AACAP,YAAAA,SAAS,GAAGC,UAAZ;AACD;AACF,SAZD,MAYO;AACL;AACD;AACF;;AAED,UAAID,SAAJ,EAAe;AACb;AACAD,QAAAA,KAAK,CAAC,CAAD,CAAL,CAASiB,GAAT,GAAeL,MAAM,CAACC,MAAP,CAAc,EAAd,EAAkBhB,MAAM,CAACI,SAAD,CAAN,CAAkB,CAAlB,EAAqBiB,KAAvC,CAAf,CAFa,CAEgD;;AAE7Dd,QAAAA,UAAU,GAAGP,MAAM,CAACsB,KAAP,CAAalB,SAAb,EAAwBF,KAAxB,CAAb;AACAK,QAAAA,UAAU,CAACgB,OAAX,CAAmBpB,KAAnB;AACAL,QAAAA,MAAM,CAACE,MAAD,EAASI,SAAT,EAAoBF,KAAK,GAAGE,SAAR,GAAoB,CAAxC,EAA2CG,UAA3C,CAAN;AACD;AACF;AACF;;AAED,SAAO,CAACE,IAAR;AACD;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,SAASQ,UAAT,CAAoBjB,MAApB,EAA4BwB,UAA5B,EAAwC;AACtC,QAAMC,KAAK,GAAGzB,MAAM,CAACwB,UAAD,CAAN,CAAmB,CAAnB,CAAd;AACA,QAAME,OAAO,GAAG1B,MAAM,CAACwB,UAAD,CAAN,CAAmB,CAAnB,CAAhB;AACA,MAAIG,aAAa,GAAGH,UAAU,GAAG,CAAjC;AACA;;AAEA,QAAMI,cAAc,GAAG,EAAvB;AACA,QAAMC,SAAS,GACbJ,KAAK,CAACb,UAAN,IAAoBc,OAAO,CAACI,MAAR,CAAeL,KAAK,CAACX,WAArB,EAAkCW,KAAK,CAACJ,KAAxC,CADtB;AAEA,QAAMU,WAAW,GAAGF,SAAS,CAAC7B,MAA9B;AACA;;AAEA,QAAMC,KAAK,GAAG,EAAd;AACA;;AAEA,QAAM+B,IAAI,GAAG,EAAb;AACA;;AAEA,MAAIC,MAAJ;AACA;;AAEA,MAAIC,QAAJ;AACA,MAAIhC,KAAK,GAAG,CAAC,CAAb;AACA;;AAEA,MAAIiC,OAAO,GAAGV,KAAd;AACA,MAAIW,MAAM,GAAG,CAAb;AACA,MAAIf,KAAK,GAAG,CAAZ;AACA,QAAMgB,MAAM,GAAG,CAAChB,KAAD,CAAf,CA5BsC,CA4Bf;AACvB;;AAEA,SAAOc,OAAP,EAAgB;AACd;AACA,WAAOnC,MAAM,CAAC,EAAE2B,aAAH,CAAN,CAAwB,CAAxB,MAA+BQ,OAAtC,EAA+C,CAC7C;AACD;;AAEDP,IAAAA,cAAc,CAACU,IAAf,CAAoBX,aAApB;;AAEA,QAAI,CAACQ,OAAO,CAACvB,UAAb,EAAyB;AACvBqB,MAAAA,MAAM,GAAGP,OAAO,CAACa,WAAR,CAAoBJ,OAApB,CAAT;;AAEA,UAAI,CAACA,OAAO,CAACK,IAAb,EAAmB;AACjBP,QAAAA,MAAM,CAACK,IAAP,CAAY,IAAZ;AACD;;AAED,UAAIJ,QAAJ,EAAc;AACZL,QAAAA,SAAS,CAACY,UAAV,CAAqBN,OAAO,CAACd,KAA7B;AACD;;AAED,UAAIc,OAAO,CAACtB,2BAAZ,EAAyC;AACvCgB,QAAAA,SAAS,CAACa,kCAAV,GAA+C,IAA/C;AACD;;AAEDb,MAAAA,SAAS,CAACc,KAAV,CAAgBV,MAAhB;;AAEA,UAAIE,OAAO,CAACtB,2BAAZ,EAAyC;AACvCgB,QAAAA,SAAS,CAACa,kCAAV,GAA+CvB,SAA/C;AACD;AACF,KA5Ba,CA4BZ;;;AAEFe,IAAAA,QAAQ,GAAGC,OAAX;AACAA,IAAAA,OAAO,GAAGA,OAAO,CAACK,IAAlB;AACD,GA/DqC,CA+DpC;AACF;;;AAEAL,EAAAA,OAAO,GAAGV,KAAV;;AAEA,SAAO,EAAEvB,KAAF,GAAU6B,WAAW,CAACrB,MAA7B,EAAqC;AACnC,SACE;AACAqB,IAAAA,WAAW,CAAC7B,KAAD,CAAX,CAAmB,CAAnB,MAA0B,MAA1B,IACA6B,WAAW,CAAC7B,KAAK,GAAG,CAAT,CAAX,CAAuB,CAAvB,MAA8B,OAD9B,IAEA6B,WAAW,CAAC7B,KAAD,CAAX,CAAmB,CAAnB,EAAsBS,IAAtB,KAA+BoB,WAAW,CAAC7B,KAAK,GAAG,CAAT,CAAX,CAAuB,CAAvB,EAA0BS,IAFzD,IAGAoB,WAAW,CAAC7B,KAAD,CAAX,CAAmB,CAAnB,EAAsBmB,KAAtB,CAA4BuB,IAA5B,KAAqCb,WAAW,CAAC7B,KAAD,CAAX,CAAmB,CAAnB,EAAsBkB,GAAtB,CAA0BwB,IALjE,EAME;AACAvB,MAAAA,KAAK,GAAGnB,KAAK,GAAG,CAAhB;AACAmC,MAAAA,MAAM,CAACC,IAAP,CAAYjB,KAAZ,EAFA,CAEmB;;AAEnBc,MAAAA,OAAO,CAACvB,UAAR,GAAqBO,SAArB;AACAgB,MAAAA,OAAO,CAACD,QAAR,GAAmBf,SAAnB;AACAgB,MAAAA,OAAO,GAAGA,OAAO,CAACK,IAAlB;AACD;AACF,GAnFqC,CAmFpC;;;AAEFX,EAAAA,SAAS,CAAC7B,MAAV,GAAmB,EAAnB,CArFsC,CAqFhB;AACtB;AACA;;AAEA,MAAImC,OAAJ,EAAa;AACX;AACAA,IAAAA,OAAO,CAACvB,UAAR,GAAqBO,SAArB;AACAgB,IAAAA,OAAO,CAACD,QAAR,GAAmBf,SAAnB;AACD,GAJD,MAIO;AACLkB,IAAAA,MAAM,CAACQ,GAAP;AACD,GA/FqC,CA+FpC;AACF;;;AAEA3C,EAAAA,KAAK,GAAGmC,MAAM,CAAC3B,MAAf;;AAEA,SAAOR,KAAK,EAAZ,EAAgB;AACd,UAAMoB,KAAK,GAAGS,WAAW,CAACT,KAAZ,CAAkBe,MAAM,CAACnC,KAAD,CAAxB,EAAiCmC,MAAM,CAACnC,KAAK,GAAG,CAAT,CAAvC,CAAd;AACA,UAAMmB,KAAK,GAAGO,cAAc,CAACiB,GAAf,EAAd;AACA5C,IAAAA,KAAK,CAACsB,OAAN,CAAc,CAACF,KAAD,EAAQA,KAAK,GAAGC,KAAK,CAACZ,MAAd,GAAuB,CAA/B,CAAd;AACAZ,IAAAA,MAAM,CAACE,MAAD,EAASqB,KAAT,EAAgB,CAAhB,EAAmBC,KAAnB,CAAN;AACD;;AAEDpB,EAAAA,KAAK,GAAG,CAAC,CAAT;;AAEA,SAAO,EAAEA,KAAF,GAAUD,KAAK,CAACS,MAAvB,EAA+B;AAC7BsB,IAAAA,IAAI,CAACI,MAAM,GAAGnC,KAAK,CAACC,KAAD,CAAL,CAAa,CAAb,CAAV,CAAJ,GAAiCkC,MAAM,GAAGnC,KAAK,CAACC,KAAD,CAAL,CAAa,CAAb,CAA1C;AACAkC,IAAAA,MAAM,IAAInC,KAAK,CAACC,KAAD,CAAL,CAAa,CAAb,IAAkBD,KAAK,CAACC,KAAD,CAAL,CAAa,CAAb,CAAlB,GAAoC,CAA9C;AACD;;AAED,SAAO8B,IAAP;AACD","sourcesContent":["/**\n * @typedef {import('micromark-util-types').Token} Token\n * @typedef {import('micromark-util-types').Chunk} Chunk\n * @typedef {import('micromark-util-types').Event} Event\n */\nimport {splice} from 'micromark-util-chunked'\n\n/**\n * Tokenize subcontent.\n *\n * @param {Event[]} events\n * @returns {boolean}\n */\nexport function subtokenize(events) {\n  /** @type {Record<string, number>} */\n  const jumps = {}\n  let index = -1\n  /** @type {Event} */\n\n  let event\n  /** @type {number|undefined} */\n\n  let lineIndex\n  /** @type {number} */\n\n  let otherIndex\n  /** @type {Event} */\n\n  let otherEvent\n  /** @type {Event[]} */\n\n  let parameters\n  /** @type {Event[]} */\n\n  let subevents\n  /** @type {boolean|undefined} */\n\n  let more\n\n  while (++index < events.length) {\n    while (index in jumps) {\n      index = jumps[index]\n    }\n\n    event = events[index] // Add a hook for the GFM tasklist extension, which needs to know if text\n    // is in the first content of a list item.\n\n    if (\n      index &&\n      event[1].type === 'chunkFlow' &&\n      events[index - 1][1].type === 'listItemPrefix'\n    ) {\n      subevents = event[1]._tokenizer.events\n      otherIndex = 0\n\n      if (\n        otherIndex < subevents.length &&\n        subevents[otherIndex][1].type === 'lineEndingBlank'\n      ) {\n        otherIndex += 2\n      }\n\n      if (\n        otherIndex < subevents.length &&\n        subevents[otherIndex][1].type === 'content'\n      ) {\n        while (++otherIndex < subevents.length) {\n          if (subevents[otherIndex][1].type === 'content') {\n            break\n          }\n\n          if (subevents[otherIndex][1].type === 'chunkText') {\n            subevents[otherIndex][1]._isInFirstContentOfListItem = true\n            otherIndex++\n          }\n        }\n      }\n    } // Enter.\n\n    if (event[0] === 'enter') {\n      if (event[1].contentType) {\n        Object.assign(jumps, subcontent(events, index))\n        index = jumps[index]\n        more = true\n      }\n    } // Exit.\n    else if (event[1]._container) {\n      otherIndex = index\n      lineIndex = undefined\n\n      while (otherIndex--) {\n        otherEvent = events[otherIndex]\n\n        if (\n          otherEvent[1].type === 'lineEnding' ||\n          otherEvent[1].type === 'lineEndingBlank'\n        ) {\n          if (otherEvent[0] === 'enter') {\n            if (lineIndex) {\n              events[lineIndex][1].type = 'lineEndingBlank'\n            }\n\n            otherEvent[1].type = 'lineEnding'\n            lineIndex = otherIndex\n          }\n        } else {\n          break\n        }\n      }\n\n      if (lineIndex) {\n        // Fix position.\n        event[1].end = Object.assign({}, events[lineIndex][1].start) // Switch container exit w/ line endings.\n\n        parameters = events.slice(lineIndex, index)\n        parameters.unshift(event)\n        splice(events, lineIndex, index - lineIndex + 1, parameters)\n      }\n    }\n  }\n\n  return !more\n}\n/**\n * Tokenize embedded tokens.\n *\n * @param {Event[]} events\n * @param {number} eventIndex\n * @returns {Record<string, number>}\n */\n\nfunction subcontent(events, eventIndex) {\n  const token = events[eventIndex][1]\n  const context = events[eventIndex][2]\n  let startPosition = eventIndex - 1\n  /** @type {number[]} */\n\n  const startPositions = []\n  const tokenizer =\n    token._tokenizer || context.parser[token.contentType](token.start)\n  const childEvents = tokenizer.events\n  /** @type {[number, number][]} */\n\n  const jumps = []\n  /** @type {Record<string, number>} */\n\n  const gaps = {}\n  /** @type {Chunk[]} */\n\n  let stream\n  /** @type {Token|undefined} */\n\n  let previous\n  let index = -1\n  /** @type {Token|undefined} */\n\n  let current = token\n  let adjust = 0\n  let start = 0\n  const breaks = [start] // Loop forward through the linked tokens to pass them in order to the\n  // subtokenizer.\n\n  while (current) {\n    // Find the position of the event for this token.\n    while (events[++startPosition][1] !== current) {\n      // Empty.\n    }\n\n    startPositions.push(startPosition)\n\n    if (!current._tokenizer) {\n      stream = context.sliceStream(current)\n\n      if (!current.next) {\n        stream.push(null)\n      }\n\n      if (previous) {\n        tokenizer.defineSkip(current.start)\n      }\n\n      if (current._isInFirstContentOfListItem) {\n        tokenizer._gfmTasklistFirstContentOfListItem = true\n      }\n\n      tokenizer.write(stream)\n\n      if (current._isInFirstContentOfListItem) {\n        tokenizer._gfmTasklistFirstContentOfListItem = undefined\n      }\n    } // Unravel the next token.\n\n    previous = current\n    current = current.next\n  } // Now, loop back through all events (and linked tokens), to figure out which\n  // parts belong where.\n\n  current = token\n\n  while (++index < childEvents.length) {\n    if (\n      // Find a void token that includes a break.\n      childEvents[index][0] === 'exit' &&\n      childEvents[index - 1][0] === 'enter' &&\n      childEvents[index][1].type === childEvents[index - 1][1].type &&\n      childEvents[index][1].start.line !== childEvents[index][1].end.line\n    ) {\n      start = index + 1\n      breaks.push(start) // Help GC.\n\n      current._tokenizer = undefined\n      current.previous = undefined\n      current = current.next\n    }\n  } // Help GC.\n\n  tokenizer.events = [] // If there’s one more token (which is the cases for lines that end in an\n  // EOF), that’s perfect: the last point we found starts it.\n  // If there isn’t then make sure any remaining content is added to it.\n\n  if (current) {\n    // Help GC.\n    current._tokenizer = undefined\n    current.previous = undefined\n  } else {\n    breaks.pop()\n  } // Now splice the events from the subtokenizer into the current events,\n  // moving back to front so that splice indices aren’t affected.\n\n  index = breaks.length\n\n  while (index--) {\n    const slice = childEvents.slice(breaks[index], breaks[index + 1])\n    const start = startPositions.pop()\n    jumps.unshift([start, start + slice.length - 1])\n    splice(events, start, 2, slice)\n  }\n\n  index = -1\n\n  while (++index < jumps.length) {\n    gaps[adjust + jumps[index][0]] = adjust + jumps[index][1]\n    adjust += jumps[index][1] - jumps[index][0] - 1\n  }\n\n  return gaps\n}\n"]},"metadata":{},"sourceType":"module"}